{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f015dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ac0f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124 cuda: 12.4 is_available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__, \n",
    "      \"cuda:\", torch.version.cuda, \n",
    "      \"is_available:\", \n",
    "      torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b8ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329b068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env  파일 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655c3e3",
   "metadata": {},
   "source": [
    "# inference api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "result = client.translation(\n",
    "    \"Меня зовут Вольфганг и я живу в Берлине\",\n",
    "    model=\"Helsinki-NLP/opus-mt-ko-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac21af9",
   "metadata": {},
   "source": [
    "# pipline()\n",
    "task: 텍스트 기반한 감성분석, 분류(제로샷), 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853b274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997068047523499}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\") # task 지정\n",
    "classifier(\"Mickey, who almost died for the 17th time, is printed out in duplicate due to the complacency of the corporate organization. This is a felony. Now Mickey 17 and 18, both can be discarded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f97ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.48192381858825684}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", \"tabularisai/multilingual-sentiment-analysis\") # task 지정\n",
    "classifier(\"Mickey, who almost died for the 17th time, is printed out in duplicate due to the complacency of the corporate organization. This is a felony. Now Mickey 17 and 18, both can be discarded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11ca2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15278035402297974,\n",
       "  'token': 9638,\n",
       "  'token_str': '이',\n",
       "  'sequence': '안녕하세요? 나는 이 모델입니다.'},\n",
       " {'score': 0.10853645205497742,\n",
       "  'token': 62592,\n",
       "  'token_str': '여자',\n",
       "  'sequence': '안녕하세요? 나는 여자 모델입니다.'},\n",
       " {'score': 0.07730689644813538,\n",
       "  'token': 108399,\n",
       "  'token_str': '가수',\n",
       "  'sequence': '안녕하세요? 나는 가수 모델입니다.'},\n",
       " {'score': 0.07643700391054153,\n",
       "  'token': 9283,\n",
       "  'token_str': '모',\n",
       "  'sequence': '안녕하세요? 나는 모 모델입니다.'},\n",
       " {'score': 0.03228118270635605,\n",
       "  'token': 102574,\n",
       "  'token_str': '프로',\n",
       "  'sequence': '안녕하세요? 나는 프로 모델입니다.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"fill-mask\", \"bert-base-multilingual-cased\") # task 지정\n",
    "classifier(\"안녕하세요? 나는 [MASK] 모델입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"question-answering\" 질문대답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8560d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", \"fka/awesome-chatgpt-prompts\") # task 지정\n",
    "question_answerer(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '오늘 날씨 어때요?\\n오늘은 어제보다는 기온이 조금 떨어지겠습니다.\\n아~ 어~ 낮기온은 서울이 11도 등 전국이 영상권에 머물겠습니다.\\n다만 동해안 지방은 오늘 낮부터 바람이 강하게 불겠고 동해안 지방에도 건조 주의보가 발효 중인 만큼 불씨 관리 잘해주시기 바랍니다.\\n내일도 전국이 대체로 맑겠습니다.\\n서울이 12도 대구는 16도 등으로 오늘보다 기온이 크게 오르겠습니다.\\n낮기온은 서울이 17도 등 전국이 영상권에 머물겠고 강릉은 18도로 오늘과 비슷하겠습니다.\\n울릉도와 독도는 흐린 가운데 비가 조금 내리겠습니다.\\n물결은 동해 먼바다에서 최고 4m까지 높게 일겠습니다.\\n다음 주 화요일에는 전국에 비가 내릴 전망입니다. 오늘(21일)은 전국에 구름이 많다가 낮부터 점차 흐려지겠습니다.\\n하지만 동해안은 오전까지 비가 내리겠고, 동해안은 낮부터 비가 내리겠습니다.\\n강원 영동 남부와 경북 동해안, 경북 동해안은 동풍의 영향으로 비가 내리겠습니다.\\n오늘 낮 최고기온은 서울이 21도, 대전 21도, 대구 22도, 전주 22도, 광주 23도 등으로 어제와 비슷하거나 조금 낮겠습니다.\\n바다의 물결은 동해 먼바다에서 최고 2.5미터로 높게 일겠고, 그 밖의 해상에서는 0.5~2.5미터의 비교적 높은 파도가 일겠습니다.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"text-generation\", \"skt/kogpt2-base-v2\") # task 지정\n",
    "question_answerer(\"오늘 날씨 어때\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"summarization\" 문장요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16547e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summ = pipeline(\"summarization\", \"Open-Orca/OpenOrca\") # task 지정\n",
    "summ(\"\"\"The novel contains more details about Nasha, who treated Mickey as a human being from the beginning. Nasha's family is a refugee and has migrated from a planet devastated by civil war, receiving discrimination and contempt.\n",
    "           Mickey, who almost died for the 17th time, is printed out in duplicate due to the complacency of the corporate organization. This is a felony. Now Mickey 17 and 18, both can be discarded.\n",
    "           However, Nasha says all 18 Mickey's were different. Like the various aspects of the original Mickey's personality that disappeared a long time ago. Additionally, all beings change by interacting with the environment and experience.\n",
    "           In the end, Expendables Mickey and Nasha, the migrants, lead the rebellion. And in this process, the film shows that no matter how much memory and DNA are duplicated, the judgments and decisions made by humans are still an uncertain subjective area.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "006fa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ner\" 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4d6a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", \"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)  # 개체명 인식\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"translation\" 한글 -> 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63750fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Seoul has a total of 30 billion dollars in these sectors and plans to build and invest funds by December 2.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translation = pipeline(\"translation\", \"Helsinki-NLP/opus-mt-ko-en\") # task 지정\n",
    "translation(\"서울시는 이들 분야에 총 300억원을 출자하고, 12월 2일까지 펀드를 결성해 투자에 나설 계획이다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"image-to-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244f3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'two birds are standing next to each other '}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "imagetotext = pipeline(\"image-to-text\", \"ydshieh/vit-gpt2-coco-en\")\n",
    "imagetotext(\"https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png\", max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312d45a",
   "metadata": {},
   "source": [
    "# fine tuning을 위한 AutoClass 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5074313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f4262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4470.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 - 모델\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def prepocess_f(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(prepocess_f, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcbee1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8477b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "from transformers import TrainingArguments, Trainer\n",
    "tr_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = tr_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec184f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7815' max='7815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7815/7815 59:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7815, training_loss=0.11893231360788767, metrics={'train_runtime': 3571.2197, 'train_samples_per_second': 35.002, 'train_steps_per_second': 2.188, 'total_flos': 1.6558424832e+16, 'train_loss': 0.11893231360788767, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c1adbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 03:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_distilbert_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_distilbert_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_distilbert_imdb\\\\vocab.txt',\n",
       " './fine_tuned_distilbert_imdb\\\\added_tokens.json',\n",
       " './fine_tuned_distilbert_imdb\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "trainer.evaluate()\n",
    "# 저장\n",
    "model_path = './fine_tuned_distilbert_imdb'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f349f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장모델 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 모델 로드\n",
    "# 모델.eval() \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27d266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  3185,  2001,  7078, 10392,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장 모델로 추론하기\n",
    "input_text = 'This movie was absolutely fantastic!'\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\") # pytorch 형식으로 텐서를 리턴\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d129d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7213,  3.1215]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad(): # 역전파x, 추론 only\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "812783f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class ID: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "print(f\"predicted class ID: {predicted_class_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47deb111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda_transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
